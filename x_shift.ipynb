{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-03T14:31:02.053281Z",
     "start_time": "2023-11-03T14:31:00.621487Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "config = {\n",
    "    'dataset': 'uklex18', # choices=['arxiv', 'drug', 'huffpost', 'mimic', 'fmow', 'yearbook']\n",
    "    'method': 'erm', # choices=['er', 'coral', 'ensemble', 'ewc', 'ft', 'groupdro', 'irm', 'si', 'erm', 'simclr', 'swav', 'swa']\n",
    "    'device': 0,  # 'gpu id'\n",
    "    'random_seed': 1,  # 'random seed number'\n",
    "\n",
    "    'eval_fix': False,\n",
    "\n",
    "    # Training hyperparameters\n",
    "    'train_update_iter': 1000,  # 'train update iter'\n",
    "    'lr': 2e-05,  # 'the base learning rate of the generator'\n",
    "    'momentum': 0.9,  # 'momentum'\n",
    "    'weight_decay': 0.01,  # 'weight decay'\n",
    "    'mini_batch_size': 60,  # 'mini batch size for SGD'\n",
    "    'reduced_train_prop': None,  # 'proportion of samples allocated to train at each time step'\n",
    "    'reduction': 'mean',\n",
    "    'eval_freq': 50,\n",
    "    'patience': 3,\n",
    "\n",
    "    # Evaluation\n",
    "    'offline': False,  # help='evaluate offline at a single time step split'\n",
    "    'difficulty': False,  # 'task difficulty'\n",
    "    # todo: set value of split_time\n",
    "    'split_time': 2008,  # 'timestep to split ID vs OOD'\n",
    "    'test_time': 2008,\n",
    "    'eval_next_timestamps': 1,  # 'number of future timesteps to evaluate on'\n",
    "    'eval_worst_time': False,  # 'evaluate worst timestep accuracy'\n",
    "    'load_model': False,  # 'load trained model for evaluation only'\n",
    "    'eval_metric': 'acc',  # choices=['acc', 'f1', 'rmse']\n",
    "    'eval_all_timestamps': False,  # 'evaluate at ID and OOD time steps'\n",
    "\n",
    "    # ER\n",
    "    'replay_freq': 50,  # 'number of previous timesteps to finetune on'\n",
    "\n",
    "    # GroupDRO\n",
    "    'num_groups': 3,  # 'number of windows for Invariant Learning baselines'\n",
    "    'group_size': 2,  # 'window size for Invariant Learning baselines'\n",
    "    'non_overlapping': False,  # 'non-overlapping time windows'\n",
    "\n",
    "    # EWC\n",
    "    'ewc_lambda': 0.5,  # help='how strong to weigh EWC-loss (\"regularisation strength\")'\n",
    "    'gamma': 1.0,  # help='decay-term for old tasks (contribution to quadratic term)'\n",
    "    'online': True,  # help='\"online\" (=single quadratic term) or \"offline\" (=quadratic term per task) EWC'\n",
    "    'fisher_n': None,  # help='sample size for estimating FI-matrix (if \"None\", full pass over dataset)'\n",
    "    'emp_FI': False,  # help='if True, use provided labels to calculate FI (\"empirical FI\"); else predicted labels'\n",
    "\n",
    "    # A-GEM\n",
    "    'buffer_size': 10,  # 'buffer size for A-GEM'\n",
    "\n",
    "    # CORAL\n",
    "    'coral_lambda': 0.5,  # 'how strong to weigh CORAL loss'\n",
    "\n",
    "    # IRM\n",
    "    'irm_lambda': 1.0,  # 'how strong to weigh IRM penalty loss'\n",
    "    'irm_penalty_anneal_iters': 0,  # 'number of iterations after which we anneal IRM penalty loss'\n",
    "\n",
    "    # Logging, saving, and testing options\n",
    "    'data_dir': './data',  # 'directory for datasets.'\n",
    "    'log_dir': './checkpoints',  # 'directory for summaries and checkpoints.'\n",
    "    'results_dir': './results',  # 'directory for summaries and checkpoints.'\n",
    "    'num_workers': 0  # 'number of workers in data generator'\n",
    "}\n",
    "from munch import DefaultMunch\n",
    "args = DefaultMunch.fromDict(config)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-03T14:31:03.309163Z",
     "start_time": "2023-11-03T14:31:03.179639Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "PREPROCESSED_FILE = 'uklex18.pkl'\n",
    "MAX_TOKEN_LENGTH = 512\n",
    "RAW_DATA_FILE = 'uk-lex18.jsonl'\n",
    "ID_HELD_OUT = 0.2\n",
    "GROUP = 7"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-03T12:56:40.670944Z",
     "start_time": "2023-11-03T12:56:40.665877Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "                 id  year                       labels  \\\n0      UKSI19750515  1975            [SOCIAL SECURITY]   \n1      UKSI19761267  1976            [SOCIAL SECURITY]   \n2      UKSI19760965  1976            [SOCIAL SECURITY]   \n3      UKSI19790628  1979            [SOCIAL SECURITY]   \n4      UKSI19821163  1982             [TRANSPORTATION]   \n...             ...   ...                          ...   \n36450  UKSI20180626  2018            [SOCIAL SECURITY]   \n36449  UKSI20180431  2018                [HEALTH CARE]   \n36448  UKSI20180061  2018            [SOCIAL SECURITY]   \n36454  UKSI20180221  2018                    [HOUSING]   \n36499  UKSI20180153  2018  [IMMIGRATION & CITIZENSHIP]   \n\n                                                   title  \\\n0      The Social Security (Guardian's Allowances) Re...   \n1      The Child Benefit and Social Security (Fixing ...   \n2           The Child Benefit (General) Regulations 1976   \n3      The Social Security (Claims and Payments) Regu...   \n4      The Motorways Traffic (England and Wales) Regu...   \n...                                                  ...   \n36450  The Scotland Act 1998 (Agency Arrangements) (S...   \n36449  The Plymouth Hospitals National Health Service...   \n36448  The Social Fund Funeral Expenses Amendment Reg...   \n36454  The Licensing of Houses in Multiple Occupation...   \n36499  The Transfer of Responsibility for Relevant Ch...   \n\n                                                    body data_type  \n0      Citation, commencement and interpretation\\n1 1...     train  \n1      Citation, commencement and interpretation\\n1 1...     train  \n2      PART I\\nGeneral\\nCitation, commencement and in...     train  \n3      P art I\\nGENERAL\\nCitation and commencement\\n1...     train  \n4      Commencement and citation\\n1\\nThese Regulation...     train  \n...                                                  ...       ...  \n36450  Citation\\n1\\nThis Order may be cited as the Sc...      test  \n36449  Citation, commencement and interpretation\\n1\\n...      test  \n36448  Citation and commencement\\n1\\nThese Regulation...      test  \n36454  Citation and Commencement\\n1\\n1\\nThis Order ma...      test  \n36499  Citation and commencement\\n1\\nThese Regulation...      test  \n\n[36500 rows x 6 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>year</th>\n      <th>labels</th>\n      <th>title</th>\n      <th>body</th>\n      <th>data_type</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>UKSI19750515</td>\n      <td>1975</td>\n      <td>[SOCIAL SECURITY]</td>\n      <td>The Social Security (Guardian's Allowances) Re...</td>\n      <td>Citation, commencement and interpretation\\n1 1...</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>UKSI19761267</td>\n      <td>1976</td>\n      <td>[SOCIAL SECURITY]</td>\n      <td>The Child Benefit and Social Security (Fixing ...</td>\n      <td>Citation, commencement and interpretation\\n1 1...</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>UKSI19760965</td>\n      <td>1976</td>\n      <td>[SOCIAL SECURITY]</td>\n      <td>The Child Benefit (General) Regulations 1976</td>\n      <td>PART I\\nGeneral\\nCitation, commencement and in...</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>UKSI19790628</td>\n      <td>1979</td>\n      <td>[SOCIAL SECURITY]</td>\n      <td>The Social Security (Claims and Payments) Regu...</td>\n      <td>P art I\\nGENERAL\\nCitation and commencement\\n1...</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>UKSI19821163</td>\n      <td>1982</td>\n      <td>[TRANSPORTATION]</td>\n      <td>The Motorways Traffic (England and Wales) Regu...</td>\n      <td>Commencement and citation\\n1\\nThese Regulation...</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>36450</th>\n      <td>UKSI20180626</td>\n      <td>2018</td>\n      <td>[SOCIAL SECURITY]</td>\n      <td>The Scotland Act 1998 (Agency Arrangements) (S...</td>\n      <td>Citation\\n1\\nThis Order may be cited as the Sc...</td>\n      <td>test</td>\n    </tr>\n    <tr>\n      <th>36449</th>\n      <td>UKSI20180431</td>\n      <td>2018</td>\n      <td>[HEALTH CARE]</td>\n      <td>The Plymouth Hospitals National Health Service...</td>\n      <td>Citation, commencement and interpretation\\n1\\n...</td>\n      <td>test</td>\n    </tr>\n    <tr>\n      <th>36448</th>\n      <td>UKSI20180061</td>\n      <td>2018</td>\n      <td>[SOCIAL SECURITY]</td>\n      <td>The Social Fund Funeral Expenses Amendment Reg...</td>\n      <td>Citation and commencement\\n1\\nThese Regulation...</td>\n      <td>test</td>\n    </tr>\n    <tr>\n      <th>36454</th>\n      <td>UKSI20180221</td>\n      <td>2018</td>\n      <td>[HOUSING]</td>\n      <td>The Licensing of Houses in Multiple Occupation...</td>\n      <td>Citation and Commencement\\n1\\n1\\nThis Order ma...</td>\n      <td>test</td>\n    </tr>\n    <tr>\n      <th>36499</th>\n      <td>UKSI20180153</td>\n      <td>2018</td>\n      <td>[IMMIGRATION &amp; CITIZENSHIP]</td>\n      <td>The Transfer of Responsibility for Relevant Ch...</td>\n      <td>Citation and commencement\\n1\\nThese Regulation...</td>\n      <td>test</td>\n    </tr>\n  </tbody>\n</table>\n<p>36500 rows × 6 columns</p>\n</div>"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data_path = os.path.join(args.data_dir, RAW_DATA_FILE)\n",
    "if not os.path.isfile(raw_data_path):\n",
    "    raise ValueError(f'{RAW_DATA_FILE} is not in the data directory {args.data_dir}!')\n",
    "\n",
    "# Load data frame from json file, group by year\n",
    "base_df = pd.read_json(raw_data_path, lines=True)\n",
    "base_df = base_df.sort_values(by=['year'])\n",
    "base_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-03T12:56:57.721627Z",
     "start_time": "2023-11-03T12:56:55.158056Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "df_years = base_df.groupby(pd.Grouper(key='data_type'))\n",
    "all_dfs = [group for _, group in df_years]\n",
    "# all_years = list(base_df['data_type'].unique())\n",
    "dfs = [all_dfs[0][:10000], all_dfs[0][10000:], all_dfs[1], all_dfs[2]]\n",
    "# years = []\n",
    "# dfs.append(pd.concat(all_dfs[:8]))\n",
    "# years.append(all_years[7])\n",
    "# all_dfs = all_dfs[8:]\n",
    "# all_years = all_years[8:]\n",
    "# for i in range(math.ceil(len(all_years)/GROUP)):\n",
    "#     try:\n",
    "#         dfs.append(pd.concat(all_dfs[GROUP*i:GROUP*i+GROUP]))\n",
    "#         years.append(all_years[GROUP*i + 1])\n",
    "#     except:\n",
    "#         dfs.append(pd.concat(all_dfs[GROUP*i:]))\n",
    "#         years.append(all_years[-1])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-03T12:57:05.769842Z",
     "start_time": "2023-11-03T12:56:58.748853Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/luke/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/luke/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "remove_these = set(stopwords.words('english') + list(string.punctuation) + list(string.digits))\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "nltk.download('punkt')\n",
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-03T14:31:12.930632Z",
     "start_time": "2023-11-03T14:31:12.009582Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import collections\n",
    "import ast"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-03T14:31:12.935763Z",
     "start_time": "2023-11-03T14:31:12.931776Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "labels_counts = {}\n",
    "for i in range(4):\n",
    "    dfs[i]['tokenized'] = dfs[i]['body'].apply(tokenizer.tokenize)\n",
    "    labels = [label for lbs in dfs[i]['tokenized'] for label in lbs]\n",
    "    all_labels = [w for w in labels if not w in remove_these]\n",
    "    labels_count = Counter(all_labels)\n",
    "    labels_counts[i+1] = labels_count"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def jaccard_set(list1, list2):\n",
    "    \"\"\"Define Jaccard Similarity function for two sets\"\"\"\n",
    "    intersection = len(list(set(list1).intersection(list2)))\n",
    "    union = (len(list1) + len(list2)) - intersection\n",
    "    return float(intersection) / union\n",
    "\n",
    "mat = []\n",
    "for k1, p in labels_counts.items():\n",
    "    sub = []\n",
    "    for k2, q in labels_counts.items():\n",
    "        kl_pq = jaccard_set(p.keys(), q.keys())\n",
    "        sub.append(kl_pq)\n",
    "    mat.append(sub)\n",
    "np.round(mat, 4)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "allowed = labels_counts[1].keys() & labels_counts[2].keys() & labels_counts[3].keys() & labels_counts[4].keys()\n",
    "\n",
    "for i in range(4):\n",
    "    entries_to_remove = labels_counts[i+1].keys() - allowed\n",
    "    for k in entries_to_remove:\n",
    "        labels_counts[i+1].pop(k, None)\n",
    "    od = collections.OrderedDict(sorted(labels_counts[i+1].items()))\n",
    "    labels_counts[i+1] = list(od.values())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for k, v in labels_counts.items():\n",
    "    plt.plot(range(0, len(v)), v, '.-', label=k)\n",
    "    # NOTE: changed `range(1, 4)` to mach actual values count\n",
    "plt.legend()  # To draw legend\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from math import log2\n",
    "from scipy.special import rel_entr, kl_div\n",
    "# calculate the jensen-shannon distance metric\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "import numpy as np\n",
    "\n",
    "# calculate the kl divergence\n",
    "def kl_divergence(p, q):\n",
    "\treturn sum(p[i] * log2(p[i]/q[i]) for i in range(len(p)))\n",
    "\n",
    "mat = []\n",
    "for k1, p in labels_counts.items():\n",
    "    sub = []\n",
    "    for k2, q in labels_counts.items():\n",
    "        # print('P =', k1, ', Q =', k2)\n",
    "        # calculate (P || Q)\n",
    "        p = np.asarray(p)\n",
    "        q = np.asarray(q)\n",
    "        kl_pq = jensenshannon(p, q, base=2)\n",
    "        sub.append(kl_pq)\n",
    "        # print('KL(P|Q): %.3f' % kl_pq)\n",
    "        # # calculate (Q || P)\n",
    "        # kl_qp = rel_entr(q, p)\n",
    "        # print('KL(Q || P): %.3f bits' % sum(kl_pq))\n",
    "    mat.append(sub)\n",
    "np.round(mat, 4)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ECTHR"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "PREPROCESSED_FILE = 'ecthr_a.pkl'\n",
    "MAX_TOKEN_LENGTH = 128\n",
    "RAW_DATA_FILE = ['ecthr-train.jsonl', 'ecthr-dev.jsonl', 'ecthr-test.jsonl']\n",
    "ID_HELD_OUT = 0.2\n",
    "GROUP = 1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-03T12:58:21.709209Z",
     "start_time": "2023-11-03T12:58:21.696746Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "base_dfs = []\n",
    "for path in RAW_DATA_FILE:\n",
    "    raw_data_path = os.path.join(args.data_dir, path)\n",
    "    if not os.path.isfile(raw_data_path):\n",
    "        raise ValueError(f'{path} is not in the data directory {args.data_dir}!')\n",
    "    base_dfs.append(pd.read_json(raw_data_path, lines=True))\n",
    "# Load data frame from json file, group by year\n",
    "# base_df = pd.concat(base_dfs)\n",
    "base_dfs[0] = base_dfs[0].sort_values(by=['judgment_date'])\n",
    "\n",
    "dfs = [base_dfs[0][:4500], base_dfs[0][4500:], base_dfs[1], base_dfs[2]]\n",
    "\n",
    "allowed = ['10', '11', '13', '14', '2', '3', '5', '6', '7', '8', '9', 'P1-1', 'P1-3', 'P4-2']\n",
    "# # allowed = ['10', '11', '13', '14', '18', '2', '3', '4', '5', '6', '7', '8', '9', 'P1-1', 'P4-2', 'P7-1', 'P7-4']\n",
    "all_dfs = []\n",
    "for base_df in base_dfs:\n",
    "    for i in range(len(base_df)):\n",
    "        new_label = []\n",
    "        for label in base_df.iloc[i, 8]:\n",
    "            if label in allowed:\n",
    "                new_label.append(label)\n",
    "        base_df.iloc[i, 8] = new_label\n",
    "    all_dfs.append(base_df)\n",
    "# base_df['year'] = pd.DatetimeIndex(base_df['judgment_date']).year\n",
    "# df_years = base_df.groupby(pd.Grouper(key='year'))\n",
    "# all_dfs = [group for _, group in df_years]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-03T12:58:48.529343Z",
     "start_time": "2023-11-03T12:58:47.066643Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q2/1cpqcsg57hvbmp5m3k_76c000000gn/T/ipykernel_68758/1379878813.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfs[i]['body'] = dfs[i]['facts'].apply(' '.join)\n",
      "/var/folders/q2/1cpqcsg57hvbmp5m3k_76c000000gn/T/ipykernel_68758/1379878813.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfs[i]['tokenized'] = dfs[i]['body'].apply(tokenizer.tokenize)\n",
      "/var/folders/q2/1cpqcsg57hvbmp5m3k_76c000000gn/T/ipykernel_68758/1379878813.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfs[i]['body'] = dfs[i]['facts'].apply(' '.join)\n",
      "/var/folders/q2/1cpqcsg57hvbmp5m3k_76c000000gn/T/ipykernel_68758/1379878813.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfs[i]['tokenized'] = dfs[i]['body'].apply(tokenizer.tokenize)\n"
     ]
    }
   ],
   "source": [
    "labels_counts = {}\n",
    "for i in range(4):\n",
    "    dfs[i]['body'] = dfs[i]['facts'].apply(' '.join)\n",
    "    dfs[i]['tokenized'] = dfs[i]['body'].apply(tokenizer.tokenize)\n",
    "    labels = [label for lbs in dfs[i]['tokenized'] for label in lbs]\n",
    "    all_labels = [w for w in labels if not w in remove_these]\n",
    "    labels_count = Counter(all_labels)\n",
    "    labels_counts[i+1] = labels_count"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-03T12:58:59.996242Z",
     "start_time": "2023-11-03T12:58:50.572089Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[1.    , 0.3341, 0.2899, 0.291 ],\n       [0.3341, 1.    , 0.2872, 0.2858],\n       [0.2899, 0.2872, 1.    , 0.3686],\n       [0.291 , 0.2858, 0.3686, 1.    ]])"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def jaccard_set(list1, list2):\n",
    "    \"\"\"Define Jaccard Similarity function for two sets\"\"\"\n",
    "    intersection = len(list(set(list1).intersection(list2)))\n",
    "    union = (len(list1) + len(list2)) - intersection\n",
    "    return float(intersection) / union\n",
    "\n",
    "mat = []\n",
    "for k1, p in labels_counts.items():\n",
    "    sub = []\n",
    "    for k2, q in labels_counts.items():\n",
    "        kl_pq = jaccard_set(p.keys(), q.keys())\n",
    "        sub.append(kl_pq)\n",
    "    mat.append(sub)\n",
    "np.round(mat, 4)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-03T12:59:14.239590Z",
     "start_time": "2023-11-03T12:59:14.230113Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "allowed = labels_counts[1].keys() & labels_counts[2].keys() & labels_counts[3].keys() & labels_counts[4].keys()\n",
    "\n",
    "for i in range(4):\n",
    "    entries_to_remove = labels_counts[i+1].keys() - allowed\n",
    "    for k in entries_to_remove:\n",
    "        labels_counts[i+1].pop(k, None)\n",
    "    od = collections.OrderedDict(sorted(labels_counts[i+1].items()))\n",
    "    labels_counts[i+1] = list(od.values())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-03T14:08:16.591531Z",
     "start_time": "2023-11-03T14:08:16.472303Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.    , 0.1729, 0.2312, 0.2359],\n       [0.1729, 0.    , 0.1405, 0.1551],\n       [0.2312, 0.1405, 0.    , 0.1458],\n       [0.2359, 0.1551, 0.1458, 0.    ]])"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import log2\n",
    "from scipy.special import rel_entr, kl_div\n",
    "# calculate the jensen-shannon distance metric\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "import numpy as np\n",
    "\n",
    "# calculate the kl divergence\n",
    "def kl_divergence(p, q):\n",
    "\treturn sum(p[i] * log2(p[i]/q[i]) for i in range(len(p)))\n",
    "\n",
    "mat = []\n",
    "for k1, p in labels_counts.items():\n",
    "    sub = []\n",
    "    for k2, q in labels_counts.items():\n",
    "        # print('P =', k1, ', Q =', k2)\n",
    "        # calculate (P || Q)\n",
    "        p = np.asarray(p)\n",
    "        q = np.asarray(q)\n",
    "        kl_pq = jensenshannon(p, q, base=2)\n",
    "        sub.append(kl_pq)\n",
    "        # print('KL(P|Q): %.3f' % kl_pq)\n",
    "        # # calculate (Q || P)\n",
    "        # kl_qp = rel_entr(q, p)\n",
    "        # print('KL(Q || P): %.3f bits' % sum(kl_pq))\n",
    "    mat.append(sub)\n",
    "np.round(mat, 4)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-03T14:08:17.753499Z",
     "start_time": "2023-11-03T14:08:17.711601Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for k, v in labels_counts.items():\n",
    "    plt.plot(range(0, len(v)), v, '.-', label=k)\n",
    "    # NOTE: changed `range(1, 4)` to mach actual values count\n",
    "plt.legend()  # To draw legend\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# EURLEX"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset multi_eurlex (/Users/luke/.cache/huggingface/datasets/multi_eurlex/default-label_level=level_1,language=en/1.0.0/5a12a7463045d4dcb12896b478c09b5a8a131a02b7e7bce059ba7ececc6584ee)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1c622a8be5db4c4c93fec2275c728d09"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset('multi_eurlex', language='en', label_level='level_1')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-03T14:31:27.535585Z",
     "start_time": "2023-11-03T14:31:25.380888Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "import re\n",
    "dfs = []\n",
    "\n",
    "splits = ['train', 'validation', 'test']\n",
    "for x in splits:\n",
    "    df = pd.DataFrame(columns=['celex_id', 'text', 'labels', 'year'])\n",
    "    for i in dataset[x]:\n",
    "        valid_months = [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \"July\", \"August\", \"September\",\n",
    "                        \"October\", \"November\", \"December\"]\n",
    "        pattern = r'(\\d{1,2})\\s*(' + '|'.join(valid_months) + ')\\s*(\\d{4})'\n",
    "        # matches = list(re.finditer(pattern, i['title'], re.IGNORECASE))\n",
    "\n",
    "        # if len(matches) > 0:\n",
    "        #     year = matches[0].group(3)\n",
    "        # else:\n",
    "        #\n",
    "        matches = list(re.finditer(pattern, i['text'], re.IGNORECASE))\n",
    "        if len(matches) > 0:\n",
    "            year = matches[0].group(3)\n",
    "        # else:\n",
    "            # print(i['celex_id'], i['text'][:100], '\\n')\n",
    "        elif i['celex_id'] == '31988R0091':\n",
    "            year = 1988\n",
    "        elif i['celex_id'] in ['31987D0594', '31987D0593']:\n",
    "            year = 1987\n",
    "\n",
    "        df.loc[len(df)] = list(i.values()) + [int(year)]\n",
    "    df = df.sort_values(by=['year'])\n",
    "    if x == 'train':\n",
    "        dfs += [df[:27500], df[27500:]]\n",
    "    else:\n",
    "        dfs.append(df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-03T14:39:07.535539Z",
     "start_time": "2023-11-03T14:36:24.833002Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "# dfs = [pd.DataFrame(dataset['train'])[:27500], pd.DataFrame(dataset['train'])[27500:], pd.DataFrame(dataset['validation']), pd.DataFrame(dataset['test'])]\n",
    "\n",
    "labels_counts = {}\n",
    "for i in range(4):\n",
    "    dfs[i]['tokenized'] = dfs[i]['text'].apply(tokenizer.tokenize)\n",
    "    labels = [label for lbs in dfs[i]['tokenized'] for label in lbs]\n",
    "    all_labels = [w for w in labels if not w in remove_these]\n",
    "    labels_count = Counter(all_labels)\n",
    "    labels_counts[i+1] = labels_count"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-03T14:49:49.464812Z",
     "start_time": "2023-11-03T14:48:54.174629Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[1.    , 0.3134, 0.2277, 0.2374],\n       [0.3134, 1.    , 0.2809, 0.2859],\n       [0.2277, 0.2809, 1.    , 0.4333],\n       [0.2374, 0.2859, 0.4333, 1.    ]])"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def jaccard_set(list1, list2):\n",
    "    \"\"\"Define Jaccard Similarity function for two sets\"\"\"\n",
    "    intersection = len(list(set(list1).intersection(list2)))\n",
    "    union = (len(list1) + len(list2)) - intersection\n",
    "    return float(intersection) / union\n",
    "\n",
    "mat = []\n",
    "for k1, p in labels_counts.items():\n",
    "    sub = []\n",
    "    for k2, q in labels_counts.items():\n",
    "        kl_pq = jaccard_set(p.keys(), q.keys())\n",
    "        sub.append(kl_pq)\n",
    "    mat.append(sub)\n",
    "np.round(mat, 4)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-03T14:49:50.708377Z",
     "start_time": "2023-11-03T14:49:49.911307Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "allowed = labels_counts[1].keys() & labels_counts[2].keys() & labels_counts[3].keys() & labels_counts[4].keys()\n",
    "\n",
    "for i in range(4):\n",
    "    entries_to_remove = labels_counts[i+1].keys() - allowed\n",
    "    for k in entries_to_remove:\n",
    "        labels_counts[i+1].pop(k, None)\n",
    "    od = collections.OrderedDict(sorted(labels_counts[i+1].items()))\n",
    "    labels_counts[i+1] = list(od.values())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-03T14:50:54.088545Z",
     "start_time": "2023-11-03T14:50:53.673682Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.    , 0.2391, 0.3321, 0.3592],\n       [0.2391, 0.    , 0.2167, 0.2648],\n       [0.3321, 0.2167, 0.    , 0.1805],\n       [0.3592, 0.2648, 0.1805, 0.    ]])"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import log2\n",
    "from scipy.special import rel_entr, kl_div\n",
    "# calculate the jensen-shannon distance metric\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# calculate the kl divergence\n",
    "def kl_divergence(p, q):\n",
    "    return sum(p[i] * log2(p[i] / q[i]) for i in range(len(p)))\n",
    "\n",
    "\n",
    "mat = []\n",
    "for k1, p in labels_counts.items():\n",
    "    sub = []\n",
    "    for k2, q in labels_counts.items():\n",
    "        # print('P =', k1, ', Q =', k2)\n",
    "        # calculate (P || Q)\n",
    "        p = np.asarray(p)\n",
    "        q = np.asarray(q)\n",
    "        kl_pq = jensenshannon(p, q, base=2)\n",
    "        sub.append(kl_pq)\n",
    "        # print('KL(P|Q): %.3f' % kl_pq)\n",
    "        # # calculate (Q || P)\n",
    "        # kl_qp = rel_entr(q, p)\n",
    "        # print('KL(Q || P): %.3f bits' % sum(kl_pq))\n",
    "    mat.append(sub)\n",
    "np.round(mat, 4)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-03T14:50:54.780750Z",
     "start_time": "2023-11-03T14:50:54.723191Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.1729\n",
      "0.2312\n",
      "0.2359 \n",
      "\n",
      "0.1729\n",
      "0.0\n",
      "0.1405\n",
      "0.1551 \n",
      "\n",
      "0.2312\n",
      "0.1405\n",
      "0.0\n",
      "0.1458 \n",
      "\n",
      "0.2359\n",
      "0.1551\n",
      "0.1458\n",
      "0.0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    print('\\n'.join(np.round(mat, 4)[:, i].astype(str)), '\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-03T14:08:50.555365Z",
     "start_time": "2023-11-03T14:08:50.521612Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "        celex_id                                               text  \\\n3406  32010D0491  COUNCIL DECISION\\nof 27 July 2009\\non the sign...   \n2499  32010R0476  COMMISSION REGULATION (EU) No 476/2010\\nof 31 ...   \n1845  32010R1183  COMMISSION REGULATION (EU) No 1183/2010\\nof 14...   \n1844  32010R1194  COMMISSION REGULATION (EU) No 1194/2010\\nof 14...   \n1843  32010D0806  DECISION OF THE EUROPEAN PARLIAMENT AND OF THE...   \n...          ...                                                ...   \n2613  32012R0122  COMMISSION IMPLEMENTING REGULATION (EU) No 122...   \n4173  32012D0483  COMMISSION DECISION\\nof 20 August 2012\\nsettin...   \n0     32012R0782  COMMISSION IMPLEMENTING REGULATION (EU) No 782...   \n3136  32013L0060  COMMISSION DIRECTIVE 2013/60/EU\\nof 27 Novembe...   \n4091  32013R0034  COMMISSION REGULATION (EU) No 34/2013\\nof 16 J...   \n\n                    labels  year  \n3406  [4, 11, 2, 5, 3, 15]  2009  \n2499           [3, 17, 15]  2010  \n1845            [2, 17, 6]  2010  \n1844        [11, 8, 18, 6]  2010  \n1843    [4, 19, 9, 18, 15]  2010  \n...                    ...   ...  \n2613         [0, 3, 17, 6]  2012  \n4173                [3, 6]  2012  \n0            [3, 2, 17, 6]  2012  \n3136             [7, 3, 8]  2013  \n4091     [17, 20, 0, 3, 6]  2013  \n\n[5000 rows x 4 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>celex_id</th>\n      <th>text</th>\n      <th>labels</th>\n      <th>year</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3406</th>\n      <td>32010D0491</td>\n      <td>COUNCIL DECISION\\nof 27 July 2009\\non the sign...</td>\n      <td>[4, 11, 2, 5, 3, 15]</td>\n      <td>2009</td>\n    </tr>\n    <tr>\n      <th>2499</th>\n      <td>32010R0476</td>\n      <td>COMMISSION REGULATION (EU) No 476/2010\\nof 31 ...</td>\n      <td>[3, 17, 15]</td>\n      <td>2010</td>\n    </tr>\n    <tr>\n      <th>1845</th>\n      <td>32010R1183</td>\n      <td>COMMISSION REGULATION (EU) No 1183/2010\\nof 14...</td>\n      <td>[2, 17, 6]</td>\n      <td>2010</td>\n    </tr>\n    <tr>\n      <th>1844</th>\n      <td>32010R1194</td>\n      <td>COMMISSION REGULATION (EU) No 1194/2010\\nof 14...</td>\n      <td>[11, 8, 18, 6]</td>\n      <td>2010</td>\n    </tr>\n    <tr>\n      <th>1843</th>\n      <td>32010D0806</td>\n      <td>DECISION OF THE EUROPEAN PARLIAMENT AND OF THE...</td>\n      <td>[4, 19, 9, 18, 15]</td>\n      <td>2010</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2613</th>\n      <td>32012R0122</td>\n      <td>COMMISSION IMPLEMENTING REGULATION (EU) No 122...</td>\n      <td>[0, 3, 17, 6]</td>\n      <td>2012</td>\n    </tr>\n    <tr>\n      <th>4173</th>\n      <td>32012D0483</td>\n      <td>COMMISSION DECISION\\nof 20 August 2012\\nsettin...</td>\n      <td>[3, 6]</td>\n      <td>2012</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>32012R0782</td>\n      <td>COMMISSION IMPLEMENTING REGULATION (EU) No 782...</td>\n      <td>[3, 2, 17, 6]</td>\n      <td>2012</td>\n    </tr>\n    <tr>\n      <th>3136</th>\n      <td>32013L0060</td>\n      <td>COMMISSION DIRECTIVE 2013/60/EU\\nof 27 Novembe...</td>\n      <td>[7, 3, 8]</td>\n      <td>2013</td>\n    </tr>\n    <tr>\n      <th>4091</th>\n      <td>32013R0034</td>\n      <td>COMMISSION REGULATION (EU) No 34/2013\\nof 16 J...</td>\n      <td>[17, 20, 0, 3, 6]</td>\n      <td>2013</td>\n    </tr>\n  </tbody>\n</table>\n<p>5000 rows × 4 columns</p>\n</div>"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs[2]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-03T14:45:53.211121Z",
     "start_time": "2023-11-03T14:45:53.192759Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dfs = [pd.DataFrame(dataset['train'])[:27500], pd.DataFrame(dataset['train'])[27500:], pd.DataFrame(dataset['validation']), pd.DataFrame(dataset['test'])]\n",
    "\n",
    "labels_counts = {}\n",
    "for i in range(4):\n",
    "    all_labels = [label for lbs in dfs[i]['labels'] for label in lbs]\n",
    "    labels_count = Counter(all_labels)\n",
    "    for k in labels_count.keys():\n",
    "        labels_count[k] = labels_count[k]/sum(labels_count.values())\n",
    "    if i == 2:\n",
    "        labels_count[66] = 0\n",
    "    od = collections.OrderedDict(sorted(labels_count.items()))\n",
    "    labels_counts[i+1] = list(od.values())\n",
    "\n",
    "from math import log2\n",
    "from scipy.special import rel_entr, kl_div\n",
    "# calculate the jensen-shannon distance metric\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "import numpy as np\n",
    "\n",
    "# calculate the kl divergence\n",
    "def kl_divergence(p, q):\n",
    "\treturn sum(p[i] * log2(p[i]/q[i]) for i in range(len(p)))\n",
    "\n",
    "mat = []\n",
    "for k1, p in labels_counts.items():\n",
    "    sub = []\n",
    "    for k2, q in labels_counts.items():\n",
    "        # print('P =', k1, ', Q =', k2)\n",
    "        # calculate (P || Q)\n",
    "        p = np.asarray(p)\n",
    "        q = np.asarray(q)\n",
    "        kl_pq = jensenshannon(p, q, base=2)\n",
    "        sub.append(kl_pq)\n",
    "        # print('KL(P|Q): %.3f' % kl_pq)\n",
    "        # # calculate (Q || P)\n",
    "        # kl_qp = rel_entr(q, p)\n",
    "        # print('KL(Q || P): %.3f bits' % sum(kl_pq))\n",
    "    mat.append(sub)\n",
    "np.round(mat, 4)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for k, v in labels_counts.items():\n",
    "    plt.plot(range(0, len(v)), v, '.-', label=k)\n",
    "    # NOTE: changed `range(1, 4)` to mach actual values count\n",
    "plt.legend()  # To draw legend\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
